---
title: "2019 Predicted Sales, dB Company and Country"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r install packages, include=FALSE}
#install.packages("pacman")
pacman::p_load(forecast,tidyverse,readxl,tseries,lmtest,xts,tibble,dplyr)
```

## Reading in the Data
This exercise seeks to forecast 2018 sales for "dB Company" and the country in which the company resides, called "Country," using ARIMA models. ARIMA, or autoregressive integrated moving average, models are constructed using a number of techniques to identify certain elements of time series data, like seasonality, in order to predict future values. The data are real-world business data, and proprietary information is shielded. The time series data frequency is 52 weeks.

The original datasets, which are real-world data, are not truly time series because they only show days on which a transaction was completed, so there are missing days.
```{r Data}
# convert sales data to daily time series
db_sales <- read_excel("dB_Sales.xlsx",sheet="dB_Sales",col_names = TRUE, col_types = NULL, na = "", skip = 0)
Country_sales <- read_excel("dB_Sales.xlsx",sheet="all_Country",col_names = TRUE, col_types = NULL, na = "", skip = 0)
colnames(db_sales) <- c("Date", "Sales")
colnames(Country_sales) <- c("Date", "Sales")
db_sales$Date <- as.Date(db_sales$Date, origin = "1899-12-30")
Country_sales$Date <- as.Date(Country_sales$Date, "%m-%d-%Y")
# fill in missing dates to make this true time series
db_sales <- db_sales %>% complete(Date = seq.Date(min(Date), max(Date), by="day"))
Country_sales <- Country_sales %>% complete(Date = seq.Date(min(Date), max(Date), by="day"))
# convert NAs to zeroes
db_sales[is.na(db_sales)] <- 0
Country_sales[is.na(Country_sales)] <- 0
head(db_sales)
head(Country_sales)
```
The data are now truly time series and show every day from the beginning of 2014 to the end of 2017. However, we've subsequently added a lot of zeroes, which could negatively impact the model. To compensate, we'll aggregate daily sales into weekly sales, and build the model from there.
```{r Data Continued}
# convert daily data to weekly
db_sales_full <- as.xts(db_sales$Sales,order.by=db_sales$Date)
db_sales_full <- apply.weekly(db_sales_full,sum)
db_sales_full <- data.frame(Date=index(db_sales_full), coredata(db_sales_full))
colnames(db_sales_full) <- c("Date", "Sales")
ytd_2018_db <- db_sales_full[210:261,]
db_sales <- db_sales_full[1:209,]
Country_sales_full <- as.xts(Country_sales$Sales,order.by=Country_sales$Date)
Country_sales_full <- apply.weekly(Country_sales_full,sum)
Country_sales_full <- data.frame(Date=index(Country_sales_full), coredata(Country_sales_full))
colnames(Country_sales_full) <- c("Date", "Sales")
ytd_2018_Country <- Country_sales_full[209:260,]
Country_sales <- Country_sales_full[1:208,]
head(db_sales)
head(Country_sales)
db_graph <- ggplot(db_sales, aes(Date, Sales)) + geom_line() + scale_x_date('Year')  + ylab("Weekly Sales") + xlab("")+ ggtitle("dB Weekly Sales Data") 
Country_graph <- ggplot(Country_sales, aes(Date, Sales)) + geom_line() + scale_x_date('Year')  + ylab("Weekly Sales") + xlab("")+ ggtitle("Country Weekly Sales Data") 
db_graph
Country_graph
```

## Search for Outliers

After the data are imported, we identify and eliminate outliers in both the dB and Country sales datasets. The idea is to remove outliers that will negatively influence the predictability of the model. Note that we could also smooth the data using a moving average to reduce the volatility. However, to encourage the most accurate predictions, each model will be constructed using the outlier-reduced data instead. The ts function helps to create a time series object in R as well as the tsclean function, which seeks and replaces outliers.

```{r dB Outliers}
db_sales_ts = ts(db_sales[, c('Sales')])
db_sales$clean_sales = tsclean(db_sales_ts)
ggplot() + geom_line(data = db_sales, aes(x = Date, y = clean_sales)) + ylab('Cleaned Sales')+ ggtitle("dB Weekly Sales Data, Cleaned") 
```

```{r Country Outliers}
Country_sales_ts = ts(Country_sales[, c('Sales')])
Country_sales$clean_sales = tsclean(Country_sales_ts)
ggplot() + geom_line(data = Country_sales, aes(x = Date, y = clean_sales)) + ylab('Cleaned Sales') + ggtitle("Country Weekly Sales Data, Cleaned") 
```

## Decomposition for ARIMA Models

Decomposition extracts components such as seasonality and cyclicity from the data to help us determine the ARIMA parameters, p (order of AR part), d (degree of differencing), and q (MA part). Specifically, if seasonality and trends are present, then we'll need to account for them by identifying lags and/or by differencing. Decomposition is achieved using the mslt function, a multiplicative version of stl to allow for multiple seasonal periods, outputs the time series into decomposed components.

Note: we will allow the ARIMA functions to implement differencing, if needed (e.g., parameter d in auto.arima() function). This will save time since we will not need to integrate the forecasted data to retrieve the actual values of the forecast.


```{r dB Decomposition}
#db_sales_clean = ts(na.omit(db_sales$clean_sales), frequency=52) # frequency (i.e., number of observations per cycle) is 52 since we have weekly data
#decomposition_db = mstl(db_sales_clean, s.window="periodic")
#nonseasonal_db_sales <- seasadj(decomposition_db) # subtract the seasonal components from the data
#plot(decomposition_db) # before seasonality is removed

db_sales_clean <- ts(na.omit(db_sales$clean_sales), frequency=52) # frequency (i.e., number of observations per cycle) is 52 since we have weekly data
decomposition_db <- mstl(db_sales_clean, s.window="periodic")
plot(decomposition_db)
#db_sales_clean <- seasadj(decomposition_db) # subtract the seasonal components from the data
#db_sales_clean <- diff(db_sales_clean)
decomposition_db <- mstl(db_sales_clean, s.window="periodic")
plot(decomposition_db)
```

```{r Country Decomposition}
#Country_sales_clean = ts(na.omit(Country_sales$clean_sales), frequency=52) # frequency (i.e., number of observations per cycle) is 52 since we have weekly data
#decomposition_Country = mstl(Country_sales_clean, s.window="periodic")
#nonseasonal_Country_sales <- seasadj(decomposition_Country) # subtract the seasonal components from the data
#plot(decomposition_Country) # before seasonality is removed
#plot(nonseasonal_Country_sales)# sales are stationary

Country_sales_clean <- ts(na.omit(Country_sales$clean_sales), frequency=52) # frequency (i.e., number of observations per cycle) is 52 since we have weekly data
decomposition_Country <- mstl(Country_sales_clean, s.window="periodic")
plot(decomposition_Country)
#Country_sales_clean <- seasadj(decomposition_Country) # subtract the seasonal components from the data
#Country_sales_clean <- diff(Country_sales_clean)
decomposition_Country <- mstl(Country_sales_clean, s.window="periodic")
plot(decomposition_Country)
```
## Stationarity Test

Decomposition takes the first differences of the data to ensure that it is stationary. ARIMA doesn't require stationarity, but we still need to confirm that the data are stationary or non-stationary after decomposition before proceding with the model. We invoke the Dickey-Fuller test to verify. The null hypothesis is that the data are non-stationary, with an alpha value of 0.05 or 5%. The p-value needs to be less than or equal to 0.05 in order to reject the null hypothesis, indicating we are at least 95% certain the alternative hypothesis, which is the data are stationary, is truer than the null hypothesis. Viewing the results, we can safely reject the null hypothesis because the p-value is 0.01.

```{r Stationariy Test}
adf.test(db_sales_clean, alternative = "stationary") # p-value: <0.01
adf.test(Country_sales_clean, alternative = "stationary") # p-value: <0.01
```
## ACF and PACF

Let's look at the ACF and PACF of the data to identify lagged values, or lags. If a series' lags correlate or repeat, then there most likely is a seasonal component to the series, which we'll want to account for. Identifying the lags helps to indicate which parameters (p, d, and/or q), or even (P, D, and/or Q) for the season component we'll add to the ARIMA models, to adjust.

The ACF shows a spike at lag 1, and the PACF shows spikes around a number of lags, so this can be accounted for the in the models if necessary. Otherwise, the peaks look relatively flat, and there's no obvious trend (e.g., sinusoidal characteristic).

```{r dB ACF and PACF}
Acf(db_sales_clean, main='ACF')
Pacf(db_sales_clean, main='PACF')
```

```{r Country ACF and PACF}
Acf(Country_sales_clean, main='ACF')
Pacf(Country_sales_clean, main='PACF')
```

## Building ARIMA Models

Now we can build an ARIMA model using the auto.arima function, which automatically "interprets" the data and assigned p, d, and q parameters. Observing the ACF and PACF of the the auto ARIMA fit helps us to determine if the parameters need to be tweaked. The auto.arima function shows fairly good ACF and PACF plots. However, none of the model's parameters are statistically significant; thus, further adjustment of P, D, and Q parameters are required. In general, though it is not the case here, high peaks are usually indicators of significant autocorrelation at those lags, which is something we must either eliminate or reduce as much as possible.

```{r dB ARIMA}
db_arima <- auto.arima(db_sales_clean, seasonal=TRUE,D=1)
db_arima
tsdisplay(residuals(db_arima), lag.max=45, main='(4,0,0) Model Residuals')
db_arima2 <- arima(db_sales_clean, order=c(4,0,0),seasonal = list(order = c(1,1,0),maxit = 10000)) # maxit implemented to increase maximum number of iterations before convergence for the MLE
tsdisplay(residuals(db_arima2), lag.max=45, main='(1,0,1) Model Residuals')
coeftest(db_arima2) # check statistical significance
# view roots
plot(db_arima2)
```

Although certain peaks extend marginally beyond the boundaries, the fit is pretty good, as both values are nearly highly statistically significant.

Note that the inverse characteristic roots of the ARIMA models were plotted. The roots either lie on or in (but not beyond) the unit circle, indicating that the AIRMA model is stationary and that the standard errors should be accurate.

```{r Country ARIMA}
Country_arima <- auto.arima(Country_sales_clean, seasonal=TRUE, D=1)
Country_arima
tsdisplay(residuals(Country_arima), lag.max=45, main='(1,0,0) Model Residuals')
Country_arima2 <- arima(Country_sales_clean, order=c(1,1,0),seasonal = list(order = c(1,1,0)),method="CSS") # method parameter tells us about roots
tsdisplay(residuals(Country_arima2), lag.max=45, main='(2,0,3) Model Residuals')
coeftest(Country_arima2) # check statistical significance
# view roots
plot(Country_arima2)
```

For Country sales, all coefficients are statistically significant, and all roots are <1.

## Forecasting 2019 Sales

Now that we have ARIMA models for both dB sales and Country sales, we can forecast. The solid blue curve is the mean of the range of the expected error (i.e., the mean of the lightest blue curve, wich includes the upper and lower predicted values). We will consider the mean values as the predicted sales values for 2019.

```{r dB Forecast}
db_forecast <- forecast(db_arima2, h=52) # projecting 52 weeks
plot(db_forecast)
write.csv(db_forecast, "db_forecast.csv")
#View(as.data.frame(db_forecast$mean))
```

```{r Country Forecast}
Country_forecast <- forecast(Country_arima, h=52) # projecting 52 weeks
plot(Country_forecast)
write.csv(Country_forecast, "Country_forecast.csv")
#View(as.data.frame(Country_forecast$mean))
```
Forecasted 2018 sales are conservative: for dB Company, the predicted 2018 sales is summed at 1,721,419.83  euros, whereas Country is summed at 3,319,420.49 euros. Both models predict modest sales in 2018.

In fact, the true sales of 2018 were 1,832,070 euros for dB and 3,967,110 euros for Country.

## 2018 Model Validation
Now it's time to validate how well the model predicts. We will do this by looking at the YTD sales for both dB Company and Country to mid-April (i.e., the first 17 weeks of 2019). First, we'll aggregate the actual YTD 2019 sales into weekly data for comparison with our predicted weekly values. Not only will we compare the mean of predicted sales vs. actual, but we'll also include the models' upper and lower limits (errors) to see how far, if at all, the mean predicted curve differs from the actual curve.
```{r validate}
# below, we read in the data; the data manipulation performed here is the same as above
ytd_2019_db <- read_excel("2019_YTD_Sales.xlsx",sheet="dB",col_names = TRUE, col_types = NULL, na = "", skip = 0)
ytd_2019_Country <- read_excel("2019_YTD_Sales.xlsx",sheet="Country",col_names = TRUE, col_types = NULL, na = "", skip = 0)
colnames(ytd_2019_db) <- c("Date", "Sales")
colnames(ytd_2019_Country) <- c("Date", "Sales")
ytd_2019_db$Date <- as.Date(ytd_2019_db$Date, "%m-%d-%Y")
ytd_2019_Country$Date <- as.Date(ytd_2019_Country$Date, "%m-%d-%Y")
# fill in missing dates to make this true time series
ytd_2019_db <- ytd_2019_db %>% complete(Date = seq.Date(min(Date), max(Date), by="day"))
ytd_2019_Country <- ytd_2019_Country %>% complete(Date = seq.Date(min(Date), max(Date), by="day"))
# convert NAs to zeroes
ytd_2019_db[is.na(ytd_2019_db)] <- 0
ytd_2019_Country[is.na(ytd_2019_Country)] <- 0
ytd_2019_db <- as.xts(ytd_2019_db$Sales,order.by=ytd_2019_db$Date)
ytd_2019_db <- apply.weekly(ytd_2019_db,sum)
ytd_2019_db <- data.frame(Date=index(ytd_2019_db), coredata(ytd_2019_db))
colnames(ytd_2019_db) <- c("Date", "Sales")
ytd_2019_Country <- as.xts(ytd_2019_Country$Sales,order.by=ytd_2019_Country$Date)
ytd_2019_Country <- apply.weekly(ytd_2019_Country,sum)
ytd_2019_Country <- data.frame(Date=index(ytd_2019_Country), coredata(ytd_2019_Country))
colnames(ytd_2019_Country) <- c("Date", "Sales")

# create separate dataframes to extract mean, upper, and lower values from ARIMA model
db_forecast_chunk_mean <- db_forecast$mean[0:17]
db_forecast_df <- data.frame(Date=(ytd_2019_db$Date),coredata(db_forecast_chunk_mean))
db_forecast_chunk_upper <- db_forecast$upper[0:17]
db_forecast_chunk_lower <- db_forecast$lower[0:17]
db_forecast_df$Upper <- db_forecast_chunk_upper
db_forecast_df$Lower <- db_forecast_chunk_lower
# plot predicted vs. actual with error bars
ggplot() +
  geom_line(data=ytd_2019_db, aes(x=Date, y=Sales, colour = "2019 YTD")) +
  geom_line(data=db_forecast_df, aes(x=Date, y=coredata.db_forecast_chunk_mean., colour = "2019 Predicted"))+ ggtitle("dB 2019 YTD vs. Predicted")+ geom_errorbar(aes(x=db_forecast_df$Date, y=NULL, ymin = db_forecast_df$Lower, ymax = db_forecast_df$Upper))
# create separate dataframes to extract mean, upper, and lower values from ARIMA model
Country_forecast_chunk_mean <- Country_forecast$mean[0:17]
Country_forecast_df <- data.frame(Date=(ytd_2019_Country$Date),coredata(Country_forecast_chunk_mean))
Country_forecast_chunk_upper <- Country_forecast$upper[0:17]
Country_forecast_chunk_lower <- Country_forecast$lower[0:17]
Country_forecast_df$Upper <- Country_forecast_chunk_upper
Country_forecast_df$Lower <- Country_forecast_chunk_lower
# plot predicted vs. actual with error bars
ggplot() +
  geom_line(data=ytd_2019_Country, aes(x=Date, y=Sales, colour = "2019 YTD")) +
  geom_line(data=Country_forecast_df, aes(x=Date, y=coredata.Country_forecast_chunk_mean., colour = "2019 Predicted"))+ ggtitle("Country 2019 YTD vs. Predicted")+ geom_errorbar(aes(x=Country_forecast_df$Date, y=NULL, ymin = Country_forecast_df$Lower, ymax = Country_forecast_df$Upper))

# calculate R-squared
ytd_2019_db$Predicted <- db_forecast_df$coredata.db_forecast_chunk_mean.
db_lm = lm(Sales ~ Predicted, data=ytd_2019_db)
summary(db_lm)$r.squared

ytd_2019_Country$Predicted <- Country_forecast_df$coredata.Country_forecast_chunk_mean.
Country_lm = lm(Sales ~ Predicted, data=ytd_2019_Country)
summary(Country_lm)$r.squared
```
So far, the weekly predicted line doesn't fit the weekly actual curve (the R-squared of the predicted weekly mean versus actual weekly values is only 2%). On the plus side, the max and min error bars for our 2019 predicted curve account for actual 2019 sales almost all the time (an exception is late March / early April, where the blue curve lies just above the red error bars). Thus, the model fails to charactertize or predict the exact movement of the 2019 data, and the model's estimate of 2018 total sales is modest.

There are three potential issues: 1) The model doesn't fit the historical data well to provide an adequate fit, 2) we're forecasting too far into the future (e.g., consider forecasting only 3-6 months as opposed to 12 months), or 3) the historical data is inadequate (i.e., both the amount and diversity of historical data to account for other influential factors are insufficient).


## References

Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on April 22, 2019.

Introduction To Forecasting with Arima in R
Ruslana Dalinina - https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials

Seasonal Periods
Robert Hyndman - https://robjhyndman.com/hyndsight/seasonal-periods
```{r unused code, include=FALSE}
#https://rstudio-pubs-static.s3.amazonaws.com/343096_90b218e393454f79a5012e7ad0913e76.html
db_sales$sales_ma = ma(db_sales$clean_sales, order=7) # using the clean count with no outliers
db_sales$sales_ma30 = ma(db_sales$clean_sales, order=30)
ggplot() +
  geom_line(data = db_sales, aes(x = Date, y = clean_sales, colour = "Sales")) +
  geom_line(data = db_sales, aes(x = Date, y = sales_ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = db_sales, aes(x = Date, y = sales_ma30, colour = "Monthly Moving Average"))  +
  ylab('Sales')


Country_sales$sales_ma = ma(Country_sales$clean_sales, order=7) # using the clean count with no outliers
Country_sales$sales_ma30 = ma(Country_sales$clean_sales, order=30)
ggplot() +
  geom_line(data = Country_sales, aes(x = Date, y = clean_sales, colour = "Sales")) +
  geom_line(data = Country_sales, aes(x = Date, y = sales_ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = Country_sales, aes(x = Date, y = sales_ma30, colour = "Monthly Moving Average"))  +
  ylab('Sales')
