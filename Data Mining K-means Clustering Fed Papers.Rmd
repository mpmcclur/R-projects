---
title: "HW_4_McClure_Matt"
author: "Matt McClure"
date: "August 10, 2018"
output: word_document
---

```{r setup}
#Load packages and data file into R.
#install.packages("factoextra")
#install.packages("rgl")
#install.packages("plot3D")
#install.packages("fpc")
library(factoextra)
library(cluster)
library(ggplot2)
library(dplyr)
library(ggplot2)
library(maps)
library(dplyr)
library(cluster)
library(rafalib)
library(rgl)
library(fpc)
fed_paper <- read.csv("C:/Users/Matt/Desktop/IST565/Datasets/fedPapers85.csv")
fed_paper2 <- read.csv("C:/Users/Matt/Desktop/IST565/Datasets/fedPapers85.csv")
#Remove second column.
fed_paper <- fed_paper[, !(names(fed_paper)%in% c("filename"))]
fed_paper2 <- fed_paper[, !(names(fed_paper)%in% c("filename"))]
#For fed_paper, make 1st column nominal, where 1 = Hamilton, 2 = Madison, 3 = Jay, and 4 = H&M, and 5 = disputed
fed_paper$author <-recode(fed_paper$author,"Hamilton"="1","Madison"="2","Jay"="3","HM"="4","dispt"="5")
#Leave fed_paper2 with the names in the "author" column -- we'll need them for labeling later.
View(fed_paper)
```
We will start with simple k-means to get a feel for the data and should need only 4 clusters. Since Madison, Hamilton, Jay, or Madison & Hamilton are the authors, we want "disputed"" to be clustered into one of the four categories.

But first, let's run the elbow test to check the potential number of clusters and then proceed with k-means clustering.
```{r Prepare elbow test}
fed_elbow <- function(k){
  return(kmeans(fed_paper, k, nstart = 25)$tot.withinss)
}
k_value <- 1:7 #arbitarily chosen; just want to get a good idea of how many potential centroids are generated.
fed_elbowValues <- purrr::map_dbl(k_value, fed_elbow)

plot(x = k_value, y = fed_elbowValues, 
     type = "b", frame = F,
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of square")
```
The elbow test indicates there are essentially 4 strong clusters -- perhaps the clusters are grouped by Hamilton, Madison, Jay, and H&M?
Let's use 4 principal clusters with the principal components analysis to visualize the k-means model.
```{r Prepare K-means}
set.seed(25) #Tried 10, 15, and 25 seeds -- will keep the number of seeds at 25 for consistency.
model_kmeans <- kmeans(fed_paper,centers = 4, nstart = 25, iter.max = 500, algorithm = "Hartigan-Wong")
#Print the centroids.
centers <- model_kmeans$centers
#This tells us the cluster sizes contain  14, 51, 5, 15. Note that the sum equals 85, which is the # rows in the data. A good sign.
#Analyze cluster centroids to understand cluster attributes:
cluster.stats(fed_paper$author, model_kmeans$cluster)
```
This tells us the inter-cluster distance is 1 and the intra-cluster distance is 5, which is the distinguishing factor between the distance of the points within clusters and the distance between the clusters themselves. This is fundamental when the algorithm computes the centroids or cluster means.
```{r Plotting K-means clusters}
#We can plot the intercluster distances and how each row is clustered together in the data. The clusters are represented as ellipses.
clusplot(fed_paper, model_kmeans$cluster, color=TRUE, shade=TRUE, 
         labels=2, lines=0)
#Now we can plot the K-means clusters using ggplot to see exactly where the authors are clustered.
plot(fed_paper$a ~ jitter(model_kmeans$cluster,1),
     pch=21,col=as.factor(fed_paper$author))
ggplot(fed_paper, aes(x=model_kmeans$cluster, y=fed_paper2$author, color = fed_paper2$author)) + geom_jitter() + labs(title = "K-Means Clustering Results",x = "Number of Clusters", y = "Author")
#Cluster 1 = disputed and H&M, cluster 2 = Hamilton, cluster 3 = Jay, and cluster 4 = Madison.
```
In the first plot using the clusplot function, note that most of the clusters are grouped by authors, but note that this plot shows only 16.68% of the point variability, which means we would likely need to show an extra dimension to view the whole geometric nature of the clustering results.
As shown in the ggplot, the k-means method indicates that the "disputed" papers are in the same cluster (cluster 1) as papers authored by Hamilton & Madison, which means we can argue that the disputed papers were written by Hamilton & Madison.

Now we can use the hierarchical clustering using the HAC method to compare results.
```{r Prepare HAC}
#Start with complete link:
fed_hac_comp <- hclust(dist(fed_paper, method = "euclidean"), method = "complete")
plot(fed_hac_comp)
#Average link:
fed_hac_avg <- hclust(dist(fed_paper, method = "euclidean"), method = "average")
plot(fed_hac_avg)
#Single link:
fed_hac_s <- hclust(dist(fed_paper, method = "euclidean"), method = "single")
plot(fed_hac_s)
```
Complete and Average links provide similar clustering. Single link shows a different hierarchy compared to the complete and average links. This is due to how single link handles the distance between clusters.
Nevertheless, each HAC link clustering method suggests four clusters distinct clusters.

```{r Viewing HAC Clusters}
#Output desirable number of clusters (4) after modeling.
fed_cut_comp <- cutree(fed_hac_comp, 4)
table(fed_cut_comp, fed_paper2$author)
#Complete-link clustering clusters disputed with H&M, like our k-means clustering above.
fed_cut_avg <- cutree(fed_hac_avg, 4)
table(fed_cut_avg, fed_paper2$author)
#Average-link clustering clusters Hamilton and Madison together. Disputed is not clustered with anything else.
fed_cut_s <- cutree(fed_hac_s, 4)
table(fed_cut_s, fed_paper2$author)
#Single-link clustering also clusters Hamilton and Madison together. Disputed is not clustered with anything else.
```
Interestingly, only complete-link clustering yielded results that agreed with our k-means clustering method above. Even after reducing the number of clusters to 2 and increasing to 5 in average- and single-link clustering, the disputed papers were not clustered with other authors. However, this arguably makes sense since complete link clustering calculates the maximum distance between the points in the clusters starting with the shorest links, or closest similarities, first. This contrasts average-link clustering since the latter calculates average pair proximity among points in different clusters.

In conclusion, the results from k-means clustering and complete-link hierarchical agglomerative clustering show the author(s) of the disputed papers are Hamilton & Madison, because both belonged to the same clusters, meaning their clustoids, or cluster means, were of similar distance with respect to distance between the function words in each paper and the distance between each cluster. However, this conclusion was very highly based on the number of clusters chosen, the seed value (for K-means), and the Euclidean complete-link formulation (for HAC). However, there is some dubiousness in this assessment because two other link formulations did not lead to this conclusion, so my results are very much based on my clustering parameters and how the chosen algorithms determined the clusters.